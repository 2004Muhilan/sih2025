	.file	"custom_md5.c"
	.option pic
	.attribute arch, "rv64i2p1_m2p0_a2p1_f2p2_d2p2_c2p0_zicsr2p0_zifencei2p0"
	.attribute unaligned_access, 0
	.attribute stack_align, 16
	.text
	.align	1
	.globl	md5_transform
	.type	md5_transform, @function
md5_transform:
.LFB14:
	.cfi_startproc
	addi	sp,sp,-16
	.cfi_def_cfa_offset 16
	sd	s0,8(sp)
	.cfi_offset 8, -8
	lw	a6,8(a0)
	lw	a7,12(a0)
	lbu	a2,1(a1)
	lbu	a5,2(a1)
	lw	a3,4(a0)
	lbu	t5,0(a1)
	lbu	t4,3(a1)
	lw	t1,0(a0)
	xor	a4,a6,a7
	slliw	a5,a5,16
	slliw	a2,a2,8
	or	a2,a2,a5
	and	a4,a4,a3
	lbu	a5,5(a1)
	lbu	t3,6(a1)
	or	a2,a2,t5
	slliw	t4,t4,24
	xor	a4,a7,a4
	or	a2,a2,t4
	addw	a4,a4,t1
	lbu	t5,4(a1)
	lbu	t4,7(a1)
	addw	a4,a4,a2
	slliw	a2,a4,7
	slliw	t3,t3,16
	srliw	a4,a4,25
	slliw	a5,a5,8
	or	a2,a2,a4
	or	a5,a5,t3
	addw	a2,a2,a3
	xor	t3,a6,a3
	or	a5,a5,t5
	slliw	t4,t4,24
	lbu	t6,10(a1)
	lbu	a4,9(a1)
	or	a5,a5,t4
	and	t3,t3,a2
	xor	t3,a6,t3
	addw	a5,a5,a7
	lbu	t0,8(a1)
	lbu	t4,11(a1)
	addw	a5,a5,t3
	slliw	t5,a5,7
	slliw	t3,t6,16
	srliw	a5,a5,25
	slliw	a4,a4,8
	or	t5,t5,a5
	or	a4,a4,t3
	addw	t5,a2,t5
	xor	t3,a3,a2
	or	a4,a4,t0
	slliw	t4,t4,24
	lbu	t6,14(a1)
	lbu	a5,13(a1)
	or	a4,a4,t4
	and	t3,t3,t5
	xor	t3,a3,t3
	addw	a4,a4,a6
	lbu	t2,12(a1)
	lbu	t0,15(a1)
	addw	a4,a4,t3
	slliw	t4,a4,7
	slliw	t3,t6,16
	srliw	a4,a4,25
	slliw	a5,a5,8
	or	t4,t4,a4
	or	a5,a5,t3
	addw	t4,t5,t4
	xor	t3,t5,a2
	or	a5,a5,t2
	slliw	t0,t0,24
	lbu	a4,17(a1)
	lbu	t6,18(a1)
	or	a5,a5,t0
	and	t3,t3,t4
	xor	t3,t3,a2
	addw	a5,a5,a3
	lbu	t2,16(a1)
	lbu	t0,19(a1)
	addw	a5,a5,t3
	slliw	t3,a5,7
	slliw	t6,t6,16
	srliw	a5,a5,25
	slliw	a4,a4,8
	or	t3,t3,a5
	or	a4,a4,t6
	addw	t3,t4,t3
	xor	t6,t4,t5
	or	a4,a4,t2
	slliw	t0,t0,24
	lbu	t2,22(a1)
	lbu	a5,21(a1)
	and	t6,t6,t3
	or	a4,a4,t0
	addw	a4,a4,a2
	xor	a2,t6,t5
	lbu	s0,20(a1)
	lbu	t0,23(a1)
	addw	a4,a4,a2
	slliw	t6,t2,16
	slliw	a2,a4,7
	slliw	a5,a5,8
	srliw	a4,a4,25
	or	a2,a2,a4
	or	a5,a5,t6
	addw	a2,t3,a2
	xor	t6,t3,t4
	or	a5,a5,s0
	slliw	t0,t0,24
	lbu	t2,26(a1)
	lbu	a4,25(a1)
	and	t6,t6,a2
	or	a5,a5,t0
	addw	a5,a5,t5
	xor	t5,t6,t4
	lbu	s0,24(a1)
	addw	a5,a5,t5
	lbu	t0,27(a1)
	slliw	t6,t2,16
	slliw	t5,a5,7
	slliw	a4,a4,8
	srliw	a5,a5,25
	or	t5,t5,a5
	or	a4,a4,t6
	addw	t5,a2,t5
	xor	t6,a2,t3
	or	a4,a4,s0
	slliw	t0,t0,24
	lbu	t2,30(a1)
	lbu	a5,29(a1)
	and	t6,t6,t5
	or	a4,a4,t0
	addw	a4,a4,t4
	xor	t4,t6,t3
	lbu	s0,28(a1)
	lbu	t0,31(a1)
	addw	a4,a4,t4
	slliw	t6,t2,16
	slliw	t4,a4,7
	slliw	a5,a5,8
	srliw	a4,a4,25
	or	t4,t4,a4
	or	a5,a5,t6
	addw	t4,t5,t4
	xor	t6,a2,t5
	or	a5,a5,s0
	slliw	t0,t0,24
	lbu	t2,34(a1)
	lbu	a4,33(a1)
	and	t6,t6,t4
	or	a5,a5,t0
	addw	a5,a5,t3
	xor	t3,t6,a2
	lbu	s0,32(a1)
	lbu	t0,35(a1)
	addw	a5,a5,t3
	slliw	t6,t2,16
	slliw	t3,a5,7
	slliw	a4,a4,8
	srliw	a5,a5,25
	or	t3,t3,a5
	or	a4,a4,t6
	addw	t3,t4,t3
	xor	t6,t5,t4
	or	a4,a4,s0
	slliw	t0,t0,24
	lbu	t2,38(a1)
	lbu	a5,37(a1)
	and	t6,t6,t3
	or	a4,a4,t0
	addw	a4,a4,a2
	xor	a2,t6,t5
	lbu	s0,36(a1)
	lbu	t0,39(a1)
	addw	a4,a4,a2
	slliw	t6,t2,16
	slliw	a2,a4,7
	slliw	a5,a5,8
	srliw	a4,a4,25
	or	a2,a2,a4
	or	a5,a5,t6
	addw	a2,t3,a2
	xor	t6,t4,t3
	or	a5,a5,s0
	slliw	t0,t0,24
	lbu	t2,42(a1)
	lbu	a4,41(a1)
	and	t6,t6,a2
	or	a5,a5,t0
	addw	a5,a5,t5
	xor	t5,t6,t4
	lbu	s0,40(a1)
	lbu	t0,43(a1)
	addw	a5,a5,t5
	slliw	t6,t2,16
	slliw	t5,a5,7
	slliw	a4,a4,8
	srliw	a5,a5,25
	or	t5,t5,a5
	or	a4,a4,t6
	addw	t5,a2,t5
	xor	t6,t3,a2
	or	a4,a4,s0
	slliw	t0,t0,24
	lbu	t2,46(a1)
	lbu	a5,45(a1)
	and	t6,t6,t5
	or	a4,a4,t0
	addw	a4,a4,t4
	xor	t4,t6,t3
	lbu	s0,44(a1)
	lbu	t0,47(a1)
	addw	a4,a4,t4
	slliw	t6,t2,16
	slliw	t4,a4,7
	slliw	a5,a5,8
	srliw	a4,a4,25
	or	t4,t4,a4
	or	a5,a5,t6
	addw	t4,t5,t4
	xor	t6,a2,t5
	or	a5,a5,s0
	slliw	t0,t0,24
	lbu	t2,50(a1)
	lbu	a4,49(a1)
	and	t6,t6,t4
	or	a5,a5,t0
	addw	a5,a5,t3
	xor	t3,t6,a2
	lbu	s0,48(a1)
	lbu	t0,51(a1)
	addw	a5,a5,t3
	slliw	t6,t2,16
	slliw	t3,a5,7
	slliw	a4,a4,8
	srliw	a5,a5,25
	or	t3,t3,a5
	or	a4,a4,t6
	addw	t3,t4,t3
	xor	t6,t5,t4
	or	a4,a4,s0
	slliw	t0,t0,24
	lbu	t2,54(a1)
	lbu	a5,53(a1)
	or	a4,a4,t0
	and	t6,t6,t3
	addw	a4,a4,a2
	xor	a2,t6,t5
	lbu	s0,52(a1)
	lbu	t0,55(a1)
	addw	a4,a4,a2
	slliw	t6,t2,16
	slliw	a2,a4,7
	slliw	a5,a5,8
	srliw	a4,a4,25
	or	a2,a2,a4
	or	a5,a5,t6
	addw	a2,t3,a2
	or	a5,a5,s0
	slliw	t0,t0,24
	xor	t6,t4,t3
	lbu	t2,58(a1)
	lbu	a4,57(a1)
	or	a5,a5,t0
	and	t6,t6,a2
	addw	a5,a5,t5
	xor	t5,t6,t4
	lbu	s0,56(a1)
	addw	a5,a5,t5
	lbu	t0,59(a1)
	slliw	t6,t2,16
	slliw	t5,a5,7
	slliw	a4,a4,8
	srliw	a5,a5,25
	or	t5,t5,a5
	or	a4,a4,t6
	addw	t5,a2,t5
	or	a4,a4,s0
	slliw	t0,t0,24
	xor	t6,t3,a2
	lbu	a5,61(a1)
	or	a4,a4,t0
	and	t6,t6,t5
	lbu	t0,62(a1)
	addw	a4,a4,t4
	xor	t4,t6,t3
	lbu	t2,60(a1)
	lbu	t6,63(a1)
	addw	a4,a4,t4
	slliw	a1,a4,7
	slliw	t4,t0,16
	srliw	a4,a4,25
	slliw	a5,a5,8
	or	a4,a1,a4
	or	a5,a5,t4
	addw	a4,t5,a4
	or	a5,a5,t2
	slliw	t4,t6,24
	xor	a1,a2,t5
	or	a5,a5,t4
	and	a1,a1,a4
	xor	a1,a1,a2
	addw	a5,a5,t3
	addw	a5,a5,a1
	slliw	a1,a5,7
	srliw	a5,a5,25
	ld	s0,8(sp)
	.cfi_restore 8
	or	a5,a1,a5
	addw	a3,a3,a4
	addw	t1,t1,a2
	addw	a3,a3,a5
	addw	a6,a6,a4
	addw	a7,a7,t5
	sw	t1,0(a0)
	sw	a3,4(a0)
	sw	a6,8(a0)
	sw	a7,12(a0)
	addi	sp,sp,16
	.cfi_def_cfa_offset 0
	jr	ra
	.cfi_endproc
.LFE14:
	.size	md5_transform, .-md5_transform
	.align	1
	.globl	md5_init
	.type	md5_init, @function
md5_init:
.LFB15:
	.cfi_startproc
	li	a2,1732583424
	li	a3,-271732736
	li	a4,-1732583424
	li	a5,271732736
	addi	a2,a2,769
	addi	a3,a3,-1143
	addi	a4,a4,-770
	addi	a5,a5,1142
	sw	a2,0(a0)
	sw	a3,4(a0)
	sw	a4,8(a0)
	sw	a5,12(a0)
	ret
	.cfi_endproc
.LFE15:
	.size	md5_init, .-md5_init
	.ident	"GCC: (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0"
	.section	.note.GNU-stack,"",@progbits
